%!TEX root = ../main.tex
\section{Future Work}
A question arises from \cite{Deadman:Higham:Ralha:2012}: how does the algorithm scale in a shared memory multicore environment? While experimental results are shown for a wide range of matrix sizes, tests were only run sequentially or using 8 threads, the maximum number of supported by the four \intel\xeon\cpus used as testbed in that paper. The following step in the work for this dissertation will focus in studying the scalability of the C/C++ implementations on a shared memory environment using as many cores as possible (available at \uminho: 12+12 cores (\amd); or 8+8 physical cores / 16+16 bi-threaded cores (\intel)). The results provided by this analysis may help to better plan the following work stage.

The second stage of work will focus in extending the implementations to distributed memory systems using \gpus as hardware accelerators, aiming to achieve the best possible code efficiency. This will be most time consuming task, as highly tuned algorithms require deep analysis and profiling, and it is expected that the speedup of each optimization will decrease almost exponentially over time. After achieving an efficient implementation, a similar version will be implemented using a framework, preferably \gama, to compare the performance attained and the level of effort required to achieve the best results.

In the event of the second stage finishing quite than predicted, another version may be developed to explore other possible accelerators such as \intel\mics or a distributed memory \cpu only system (multi-node cluster) with explicit use of the shared memory solution.
