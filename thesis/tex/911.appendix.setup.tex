\documentclass[../thesis]{subfiles}

\begin{document}
	\chapter{Setup and Methodology}
	\label{chp:appendix:setup}

	The environmental setup and methodologies used throughout this dissertation are described in this appendix. This is meant to avoid unnecessary repetition between the chapters.

	All the tests performed for this document were performed in the \search cluster\footnote{\url{http://search.di.uminho.pt}}. For \cref{chp:multicore}, a single computational node of group 701 was used. Each node in this group contains two 8-core \intel\xeon E5-2650 processors and 64GB shared DRAM (\numa) at 1333 MHz. Each of its processors runs at a clock frequency of 2.00 GHz and has hardware support for 32 simultaneous threads with \intel Hyper-Threading. Further information regarding the hardware in these nodes is shown in \cref{tab:search:701}.

	\begin{table}[p]
		\begin{center}
			\begin{tabular}{lr}
				\hline
				Clock frequency & 2.00 GHz \\
				Cores & 8 \\
				SIMD width & 256-bit (\avx) \\
				Memory size & 64 GB \\
				\hline
				Peak DP FLOPs & 128 GigaFLOP/s \\
				Peak Memory Bandwidth & 51.2 GB/s \\ 
				\hline
			\end{tabular}
		\end{center}
		\caption{Hardware details for \search group 701 nodes. Further information available in \cite{Intel:Xeon:e5_2650,Intel:Xeon:e5_2600}.}
		\label{tab:search:701}
	\end{table}

	Nodes in the 701 group run Linux CentOS 6.2. The executables were built using \intel Composer XE 2011, compiled with \icpc 12.0.2 and linked with \intel\mkl (for optimized \blas and \lapack) and the Armadillo C++ Linear Algebra library, version 3.800.2.

	Tests in \cref{chp:mic,chp:cuda} are run a single node of group 711 containing both \gpus and an \intel\xeonphi. Nodes in this group contain two \intel\xeon E5-2670 \cpus. They also contain 64GB shared DRAM (\numa). Each processor runs at 2.60 GHz and support up to 16 simultaneous threads (Hyper-Threading). \Cref{tab:search:711} shows more hardware details.

	\begin{table}[p]
		\begin{center}
			\begin{tabular}{lr}
				\hline
				Clock frequency & 2.60 GHz \\
				Cores & 8 \\
				SIMD width & 256-bit (\avx) \\
				Memory size & 64 GB \\
				\hline
				Peak DP FLOPs & 166.4 GigaFLOP/s \\
				Peak Memory Bandwidth & 51.2 GB/s \\ 
				\hline
			\end{tabular}
		\end{center}
		\caption{Hardware details for \search group 711 nodes. Further information available in \cite{Intel:Xeon:e5_2670,Intel:Xeon:e5_2600}.}
		\label{tab:search:711}
	\end{table}

	These nodes run Linux CentOS 6.4. For the tests in \cref{chp:mic}, they also provide \intel Composer XE 2013, with \icpc 13.1.2 and \mkl, and Armadillo 3.900.7. As for \cref{chp:cuda}, executables are built using \gcc 4.4.7 and \cuda 5.0.

	Performance tests for \cref{chp:mic} were performed using an \intel\xeonphi Coprocessor 5110P. It contains 60 cores running at 1.053 GHz and 8GB of \gddr5 memory with a maximum bandwidth of 320 GB/s (see \cref{tab:5110p}).

	\begin{table}[p]
		\begin{center}
			\begin{tabular}{lr}
				\hline
				Clock frequency & 1.053 GHz \\
				Cores & 60 \\
				SIMD width & 512-bit \\
				Memory type & \gddr5 \\
				Memory size & 8 GB \\
				Memory speed & 5.0 GT/s \\
				\hline
				Peak DP FLOPs & 1.01 TeraFLOP/s \\
				Peak Memory Bandwidth & 320 GB/s \\ 
				\hline
			\end{tabular}
		\end{center}
		\caption{Hardware details for the \intel\xeonphi Coprocessor 5110P. Further information available in \cite{Intel:XeonPhi:5110P}.}
		\label{tab:5110p}
	\end{table}

	As for \cref{chp:cuda}, tests used a \nvidia Tesla K20m board, with one GK110 Kepler \gpu \cite{NVIDIA:KEPLER} and 5GB of \gddr5 global memory. The \gpu contains 13 multiprocessors and 2496 \cuda cores. Further details in \cref{tab:k20m}

	\begin{table}[p]
		\begin{center}
			\begin{tabular}{lr}
				\hline
				\gpus & $1\times$ GK110 \\
				Multiprocessors & $13\times$ \smx \\
				\cuda cores & 192 per \smx \\
				Double-precision units & 64 per \smx \\
				\sfus & 32 per \smx \\
				Load/Store units & 32 per \smx \\
				Memory size & 5 GB \\
				\cuda capability & 3.5 \\
				\hline
				Peak DP FLOPS & 1.17 TeraFLOP/s \\
				Peak Memory bandwidth & 208 GB/s \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Hardware details for the \nvidia Tesla K20m. Further information available in \cite{NVIDIA:KEPLER,NVIDIA:TeslaKSeriesOverview}.}
		\label{tab:k20m}
	\end{table}

	All the tests in this dissertation followed the same methodology: the best 3 measurements with a tolerance of 5\%, with a minimum of 10 runs and a maximum of 20. Time measurements were confined to the implementation of the algorithm, disregarding initialization and cleanup steps (such as I/O operations, allocating and freeing memory or interpreting program options).

	Matrices of three different dimensions were used in performance tests: 2000, 4000 and 8000. Only the smallest size fits in the aggregated last-level caches of the computational nodes used
	. The remaining dimensions force the program to use \dram. \Cref{chp:multicore} also uses equivalent power of two dimensions, but due to the sensitivity demonstrated by the obtained results, these are not used in the other chapters.

	For implementations of the block method, the best block dimension was determined experimentally. For both \cref{chp:multicore,chp:cuda}, this dimension was found to be 64, while for \cref{chp:mic} it is 32.
\end{document}
