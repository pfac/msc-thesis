\documentclass[../thesis]{subfiles}

\begin{document}
	\section{Optimizations}
	\label{sec:mic:optims}

	The results presented in \cref{subsec:mic:native:results} are surprising, given the successful results obtained with the multicore implementation in \cref{chp:multicore} and the resources available in the \intel\xeonphi Coprocessor. The discrepancy is so big that a decision was made at this point to improve the performance of this implementation before exploring any other execution modes or programming models.

	Documents from Intel state the best way to prepare for \intel\xeonphi coprocessors is to fully exploit the performance that an application can get on \intel\xeon processors first. Trying to use the coprocessor without maximizing the use of parallelism on the processor will almost certainly be a disappointment \cite{Intel:MIC:Overview}.

	The optimizations presented in this section are focused on a deeper analysis of the implemented algorithm and profiling the application running on a multicore environment, as doing so allows to use the tools made available in \intel Parallel Studio XE 2013.

	\subsection{Massive Parallelism}
	Many-core devices like the \intel\xeonphi coprocessor make hundreds of parallel computing resources available to applications. When the degree of parallelism in such applications is too low, some of these resources remain idle during execution, hampering efficiency. This is a corollary from Amdahl's Law \cite{Amdahl:1967}, which explains that the maximum theoretical speedup an application might achieve in a given architecture is limited by the amount of time a sequential processor would spend in the parallel part of the code in comparison to the sequential part.

	The degree of parallelism explored so far in the matrix square root algorithm is quite limiting. For any matrix of dimension $n$, there will be at most $n$ elements to be computed in parallel, which happens only once. After the main diagonal, every other diagonal has one less element to compute, until the last diagonal where only one element exists to be computed. Consequently, the last diagonals are unable to take advantage of a high quantity of parallel resources.

	Yet, this can be compensated when solving dependencies. Analysing \cref{eq:sqrtm:diagN} shows a sum of multiplications that translates into a dot product between the elements at the left and below the one being computed (excluding the main diagonal). A dot product is a highly parallel operation implemented in Level 1 \blas, and the number of dependencies increases as the algorithm progresses. For the last diagonal's only element, this operation can perform $n-2$ multiplications in parallel and sum all the products with a reduction.

	For the point method, introducing parallelism when solving the dependencies requires a nested \texttt{parallel for} OpenMP directive with a reduction clause. As for the block method, this is extension is not trivial as most OpenMP libraries do not allow reductions to be performed using non-scalar types such as Armadillo matrices. This can be circumvented by separating the directives. Inside the parallel zone, each thread initializes a local matrix with the size of a full-block. A parallel loop iterates over the dependencies, each thread subtracting a block from the local copy. After the loop, an OpenMP \texttt{critical} directive allows each thread to add its local copy to the final result.

	This optimization does not solve the application bottlenecks, but it allows for a better usage of resources in massively parallel devices such as the \intel\xeonphi and \gpus.

	\subsection{Loop Unrolling}
	Revisiting \cref{chp:case,alg:multicore:diagonal:point,alg:multicore:diagonal:block}, in particular the description of the algorithm dependencies, a deeper analysis allows to conclude that it is logical to unroll the diagonal loop. In both algorithms, the first and second diagonals act differently from the rest. The first diagonal has no dependencies and, as such, recursively applies the square root on the focused element/block (standard \texttt{sqrt} in the point method, which in turn is used by the block method).

	On the other hand, the elements/blocks in the second diagonal depend only of those in the main diagonal. As such, there are no dependencies to solve, as the main diagonal elements/blocks are used directly to compute the final result.

	The following diagonals perform additional work, having to compute how the elements/blocks on the left and below affect the input value, where this affected value is the one used to compute the final result.

	\begin{algorithm}[htp]
		\caption[Matrix Square Root Unrolled (diagonal, point)]{Matrix Square Root (diagonal, point)}
		\label{alg:mic:diagonal:point}
		\DontPrintSemicolon

		\SetKwFunction{maind}{sqrtm\_d0}
		\SetKwFunction{firstd}{sqrtm\_d1}
		\SetKwFunction{otherd}{sqrtm\_dn}

		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;
		fill $U$ with zeros\;

		\maind{$T$, $U$}\;
		\firstd{$T$, $U$}\;
		\For{$d \leftarrow 2$ \KwTo $n-1$}{
			\otherd{$d$, $T$, $U$}\;
		}
	\end{algorithm}

	\Cref{alg:mic:diagonal:point} shows the unrolled algorithm for the point method, using three distinct functions, one for each case. \texttt{sqrtm\_d0} handles the main diagonal ($d = 0$), \texttt{sqrtm\_d1} handles the first super-diagonal ($d = 1$) and \texttt{sqrtm\_dn} handles all the other diagonals, ($d$ is provided as an argument in the function call). These functions are described in \cref{alg:mic:diagonal:point:0,alg:mic:diagonal:point:1,alg:mic:diagonal:point:n}, respectively. \Cref{alg:mic:diagonal:block:0,alg:mic:diagonal:block:1,alg:mic:diagonal:block:n} show the corresponding algorithms for the block method, following the same index expansion logic described in \cref{chp:multicore}.

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- main diagonal (point)}
		\label{alg:mic:diagonal:point:0}
		\DontPrintSemicolon

		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;

		\For{$e \leftarrow 0$ \KwTo $n-1$}{
			$U_{ee} \leftarrow \sqrt{T_{ee}}$\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- first super-diagonal (point)}
		\label{alg:mic:diagonal:point:1}
		\DontPrintSemicolon

		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;

		\For{$e \leftarrow 0$ \KwTo $n-2$}{
			$i \leftarrow e$\;
			$j \leftarrow e + 1$\;
			$U_{ij} \leftarrow \frac{T_{ij}}{U_{ii}U_{jj}}$\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- other super-diagonals (point)}
		\label{alg:mic:diagonal:point:n}
		\DontPrintSemicolon

		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{The diagonal index $d$}
		\Input{A real upper triangular matrix $T$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;

		\For{$e \leftarrow 0$ \KwTo $n-d-1$}{
			$i \leftarrow e$\;
			$j \leftarrow e + d$\;
			$r \leftarrow$ sub-row in $i$ from $i+1$ to $j-1$\;
			$c \leftarrow$ sub-column in $j$ from $i+1$ to $j-1$\;
			$s \leftarrow r \times c$\;
			$U_{ij} \leftarrow \frac{T_{ij} - s}{U_{ii} \cdot U_{jj}}$\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption[Matrix Square Root Unrolled (diagonal, block)]{Matrix Square Root (diagonal, block)}
		\label{alg:mic:diagonal:block}
		\DontPrintSemicolon

		\SetKwFunction{maind}{sqrtm\_d0}
		\SetKwFunction{sqrtm}{sqrtm\_d1}
		\SetKwFunction{sqrtm}{sqrtm\_dn}
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Input{The dimension of a full block $b$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;
		$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;
		fill $U$ with zeros\;

		\maind{$T$, $\#\mathrm{blocks}$, $b$, $U$}\;
		\firstd{$T$, $\#\mathrm{blocks} - 1$, $b$, $U$}\;
		\For{$d \leftarrow 2$ \KwTo $\#\mathrm{blocks}-1$}{
			\otherd{$d$, $T$, $\#\mathrm{blocks} - d$, $b$, $U$}\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- main diagonal (block)}
		\label{alg:mic:diagonal:block:0}
		\DontPrintSemicolon

		\SetKwFunction{min}{min}
		\SetKwFunction{range}{range}
		\SetKwFunction{sqrtm}{sqrtm}
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
		\Input{The dimension of a full block $b$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;

		\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
			$i_0 \leftarrow e \cdot b$\;
			$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
			$i \leftarrow $\range{$i_0$, $i_1$}\;
			$U_{ii} \leftarrow $\sqrtm{${T_{ii}}$}\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- first super-diagonal (block)}
		\label{alg:mic:diagonal:block:1}
		\DontPrintSemicolon

		\SetKwFunction{min}{min}
		\SetKwFunction{range}{range}
		\SetKwFunction{sylvester}{sylvester}
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
		\Input{The dimension of a full block $b$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;
		$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;

		\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
			$i_0 \leftarrow e \cdot b$\;
			$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
			$i \leftarrow $\range{$i_0$, $i_1$}\;
			$j_0 \leftarrow (e + 1) \cdot b$\;
			$j_1 \leftarrow $\min{$(e + 2) \cdot b$, $n$}$-1$\;
			$j \leftarrow $\range{$j_0$, $j_1$}\;
			$U_{ij} \leftarrow $\sylvester{$U_{ii}$, $U_{jj}$, $T_{ij}$}\;
		}
	\end{algorithm}

	\begin{algorithm}[htp]
		\caption{Matrix Square Root -- other super-diagonals (block)}
		\label{alg:mic:diagonal:block:n}
		\DontPrintSemicolon

		\SetKwFunction{min}{min}
		\SetKwFunction{range}{range}
		\SetKwFunction{sqrtm}{sqrtm}
		\SetKwFunction{sylvester}{sylvester}
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}

		\Input{A real upper triangular matrix $T$}
		\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
		\Input{The dimension of a full block $b$}
		\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
		$n \leftarrow$ dimension of $T$\;
		$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;
		
		\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
			$i_0 \leftarrow e \cdot b$\;
			$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
			$i \leftarrow $\range{$i_0$, $i_1$}\;
			\eIf{$d = 0$}{
				$U_{ii} \leftarrow $\sqrtm{${T_{ii}}$}\;
			}{
				$j_0 \leftarrow (e + d) \cdot b$\;
				$j_1 \leftarrow $\min{$(e + d + 1) \cdot b$, $n$}$-1$\;
				$j \leftarrow $\range{$j_0$, $j_1$}\;
				$F \leftarrow U_{ii}$\;
				$G \leftarrow U_{jj}$\;
				$C \leftarrow T_{ij}$\;
				\For{$z \leftarrow 1$ \KwTo $d - 1$}{
					$k_0 \leftarrow (e + z) \cdot b$\;
					$k_1 \leftarrow (e + z + 1) \cdot b - 1$\;
					$k \leftarrow $\range{$k_0$, $k_1$}\;
					$C \leftarrow C - U_{ik} \times U_{kj}$\;
				}
				$U_{ij} \leftarrow $\sylvester{$U_{ii}$, $U_{jj}$, $C$}\;
			}
		}
	\end{algorithm}

	\subsection{Armadillo}
	The Basic Hotspot analysis in \intel\vtune Amplifier XE 2013 allows to identify the most time-consuming source code regions. \Cref{tab:hotspots} shows the results of this analysis run against the code implemented so far. The two most time-consuming code regions happen in \mkl; consequently, these are ignored as the optimized library is left in charge of these operations. After \blas and \lapack, the most time-consuming function belongs to Armadillo, and is meant for copying the matrices.

	\begin{table}[htp]
		\begin{center}
			\begin{tabular}{l|r@{.}l}
				\hline
				Function & \multicolumn{2}{r}{Time Spent (s)} \\
				\hline
				\texttt{dgemm} & 43 & 311  \\
				\texttt{dtrsyl} & 23 & 398  \\
				\texttt{arrayops::copy\_big} & 15 & 555  \\
				\texttt{[OpenMP Worker]} & 6 & 303  \\
				\texttt{dgees} & 5 & 811  \\
				\hline
			\end{tabular}
		\end{center}
		\caption[Basic Hotspot analysis results]{Basic Hotspot analysis results (in development environment)}
		\label{tab:hotspots}
	\end{table}

	The Armadillo library tries to minimize matrix allocations and copies whenever possible. For example, chaining matrix addition operations use a complex system of template ``glues'' that solve these additions without performing additional memory allocations. It also cares to use \blas operations without allocating a matrix to store the result whenever possible. Yet, it is unable to perform optimizations like these when blocks are isolated because these have to be treated as standalone matrices from then on. Also, Armadillo lacks an interface for the \lapack function that solves the Sylvester equation where the result matrix is not allocated. Consequently, it is necessary to remove Armadillo from the implementation, replacing it with standard arrays and manual calls to \blas and \lapack.

	For simplicity, implementation is bound to the \intel\mkl\blas interface. In the functions arguments (\cref{alg:mic:diagonal:point,alg:mic:diagonal:point:0,alg:mic:diagonal:point:1,alg:mic:diagonal:point:n,alg:mic:diagonal:block,alg:mic:diagonal:block:0,alg:mic:diagonal:block:1,alg:mic:diagonal:block:n}), Armadillo matrices are replaced with standard memory pointers and the matrix dimension $n$. In the point method, $s \leftarrow r \times c$ is replaced with a call to \blas\texttt{DOT}, which using increments of $1$ and $n$, respectively, does not require the $c$ and $r$ arrays to be isolated.

	For the block method, $C \leftarrow C - U_{ik} \times U_{kj}$ is replaced with a call to \blas\texttt{GEMM}, and $U_{ij} \leftarrow \mathtt{sylvester} \left( U_{ii}, U_{jj}, C\right)$ is replaced with a call to \lapack\texttt{TRSYL}. Both these calls overwrite one of the operands, effectively removing any need for allocations and copy operations.

	Note that while Armadillo is removed from the computation, it is still used for I/O operations for loading the matrix from a file and outputting the result.

	\subsection{Blocks as matrices}
	\label{subsec:blockify}
	After removing Armadillo, it becomes clear how \blas and \lapack calls access sub-matrices using the leading dimensions of the whole matrix. This simple approach is, however, error prone and better locality could be achieved were it not required to jump $n$ elements from one block column to the next.

	The matrices can be reorganized so each independent block is contiguous in memory, effectively making it an independent matrix. See \cref{eq:blockify} for example. $A$ is a regular column-major matrix, with the elements in the same column contiguous in memory, and each column also contiguous in memory. When trying to access the sub-matrix $A$ corresponding to the first two rows and columns, three elements of the first column must be skipped. In matrices where the dimension is large enough this translates into one memory access per block column. ``Blockifying'' $A$ generates $B$ where this does not happen because each block is now a column-major matrix, with all the blocks in the same column contiguous in memory, and the same being true for all the columns of blocks.

	\begin{equation}
		\begin{array}{ccc}
			\left[
			\begin{array}{c|c|c|c|c}
				 1 &  2 &  4 &  7 & 11  \\
				 0 &  3 &  5 &  8 & 12  \\
				 0 &  0 &  6 &  9 & 13  \\
				 0 &  0 &  0 & 10 & 14  \\
				 0 &  0 &  0 &  0 & 15  \\
			\end{array}
			\right] & \Rightarrow & \left[
			\begin{array}{cc|cc|c}
				 1 &  2 &  4 &  7 & 11  \\
				 0 &  3 &  5 &  8 & 12  \\
				 \hline
				 0 &  0 &  6 &  9 & 13  \\
				 0 &  0 &  0 & 10 & 14  \\
				 \hline
				 0 &  0 &  0 &  0 & 15  \\
			\end{array}
			\right] \\
			A & & B
		\end{array}
		\label{eq:blockify}
	\end{equation}

	This blockify operation does add some overhead to the initialization and to the cleanup (to revert the result to standard format), but it improves cache usage by through spatial locality. This overhead may or may not be worth depending on how good is this improvement and how it affects the computation.

	\subsection{Single matrix}
	All the implementations so far assume at least two distinct matrices are used in the algorithm, one for $T$ and another one for $U$. Aside from dependencies, this implies that when a block $U_{ij}$ is being computed, another block $T_{ij}$ must also be present. The memory footprint becomes even larger with the optimization described in \cref{subsec:blockify}. ``Blockifying'' the input matrix generates a second, re-organized matrix, which is then used as the input matrix for the algorithm. The algorithm then generates a third matrix, also ``blockified'', with the result, which then has to be re-organized into a fourth standard matrix with the final result.

	\blas and \lapack calls minimize the memory footprint by overwriting one of the operands with the result of the operation. In the case of the matrix square root algorithm this is also possible since only one element/block in the input matrix $T$ is used in the computation of each element/block in $U$. Consequently, the memory footprint can be easily reduced by overwriting $T$ with $U$.
\end{document}
