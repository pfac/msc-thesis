\documentclass[../thesis]{subfiles}

\begin{document}
	\tdi{processor memory gap}
	\chapter{Technological Background}
	For more than half a century, computational systems have evolved at an increasing rate, fueled by a similar demand in computing power. The invention of the digital transistor, in 1947, smaller and faster than its predecessor, made computers smaller and more power efficient. Integrated circuits further reduced the space and power requirements of computers, which in turn led to the emergence of the microprocessor. The evolution of the microprocessor in the following decades followed had two main contributors: the number of transistors and their clock frequency.

	A processor's clock frequency is the rate at which it can issue instructions, which means that a higher frequency roughly translated in more instructions being processed each second. Increasing a processor's frequency had its impact on software development. While each generation of processors offered a faster clock frequency, programs would simply run faster without any change to the underlying source code. On the other hand, increasing the number of transistors in a chip allowed to create complex systems to optimize software execution. These focused in two critical performance techniques: the exploration of \ilp and the use of memory caches.

	In 1965, Gordon Moore, in an attempt to predict the evolution of integrated circuits during the following decade projected that the number of transistors would double every year \cite{Moore:1965}. In 1975, adding more recent data, he slowed the future rate of increase in complexity to what is still today known as Moore's Law: the number of transistors in a chip doubles every two years \cite{Moore:1975,ComputerHistory:Moore}.

	In 2003, the evolution of the microprocessor reached a milestone. The increase in clock frequency of microprocessors closely followed the increase in the number of transistors, but the power density in processors was approaching the physical limitations of silicon with air cooling techniques. Continuing to increase the frequency would require new and prohibitively expensive cooling solutions. With Moore's Law still in effect, and the lack of more \ilp to explore efficiently, most vendors now focus on creating symmetric multi-core processors for the mass market.

		\section{Parallelism}
		Parallelism is far from being a modern concept, much less in the field of \hpc. Back in the 1970s, the raw performance of vector computers marked the beginning of modern Supercomputers\cite{Strohmaier:2005:20years}. These systems, able to process multiple items of data at the same time, prevailed in supercomputer design until the beginning of the 1990s, by which time the \mpp architectures became the most popular. \mpp systems had two or more identical processors connected to separate memory and controlled by separate (but identical) operating systems. As for the middle and low-end systems consisted in \smp architectures, containing two or more identical processors connected to a single shared main memory and controlled by a single operating system. In the following years cluster systems became the dominating design. With cluster computing separate computers, each having a \smp architecture, are able to cooperate, appearing as a single system to the user. This trend has continued up to the present \cite{TheNextWave:1:2013:Supercomputers,TOP500:overtime}.

		\citewithauthor{Hennessy:Patterson:2012:CAQA} define two classes of parallelism:
			\begin{description}
				\item [Data-Level Parallelism] consists in many data items being computed at the same time;
				\item [Task-Level Parallelism], where different tasks of work are able to operate independently and largely in parallel.
			\end{description}
		The hardware can exploit these two kinds of parallelism at four different levels. Processors take advantage of \ilp without any intervention from the programmer through mechanisms like superscalarity, out-of-order execution, pipelining and speculative execution. Vector processing on the other hand, uses a single instruction to operate over a collection of data in parallel, similar to what happened with vector computers but at a smaller scale. \tlp enables both data and task level parallelism by allowing more than one thread of instructions to be executed at the same time. Threads can also be used to hide memory latencies, by allowing another thread to use the physical resources while the memory request is not fulfilled. Lastly, \rlp, used in warehouse-scale computing, exploits parallelism among largely decoupled tasks specified by the programmer or the operating system.

		The multicore advent is forcing a deep in change in software development. Legacy software, intrinsically sequential, is no longer able to profit from the evolution of computational hard as new generations of microprocessors work at roughly the same speed (sometimes even less) but provide extra resources these applications are not prepared to take advantage of. This implies a re-design of old applications, otherwise these will plateau at or near current levels of performance, facing the risk of stagnation and loss of competitiveness, both for themselves and any projects that might depend on these legacy applications \cite{Farber:2011:RedefiningWhatIsPossible}.

		\section{Heterogeneous Platforms}
		As the evolution of microprocessors move towards higher levels of parallelism, several other options exist, from using multiple machines in a cluster, allowing each to perform part of the computation independently, to specific-purpose devices such as \dsps and \gpus.

		A given system is said to be heterogeneous when it contains multiple devices of different kinds, each capable of computing several operations in parallel. The most efficient computing systems of the world in the TOP500 list\footnote{\url{http://www.top500.org}} are composed of several interconnected computing nodes, each with multiple multicore \cpus and one or more specialized hardware accelerators. \gpus and \intel\xeon Phi devices are currently the most popular.

		Popularity of these new specialized devices in \hpc, some created and developed in completely separate environments, has been increasing in the last years, triggered by the trend to use the number of cores in computing hardware. \gpus evolution, for example, where throughput is more important than speed, led to massively parallel architectures, able to process hundreds of pixels at the same time. Devices based on the \intel\mic architecture, on the other hand, are placed between \gpus and \cpus, having the characteristics for massive parallelism while still being able to handle more complex operations (such as running an operating system). Both kinds of devices are explained in more depth in \cref{chp:cuda,chp:mic} where they are explored in the context of this document's case study.

		\dsps are another class of accelerators recently made popular for \hpc due to new architectures able to perform floating-point computations. Their architecture is very similar to that of a conventional processor, both programming and memory-wise \cite{FLAWN61}. Alternatively, \fpgas mix a set of configurable logic blocks, digital signal processor blocks and traditional \cpu cores (optional), all using a configurable interconnect. The key characteristic of these devices is the ability to configure them for a specific type of computation making them extremely versatile \cite{Brodtkorb:2010}. These devices are described here for completeness, not being the scope of this document explore the case study using them.

		Most hardware accelerators suffer from the same limitations as conventional processor architectures regarding memory latency, despite the many strategies each one implements to hide or overcome the problem. Since the connection between the \cpu and an accelerator is typically performed over a \pcie interface, using the same memory banks would be a major bottleneck. For this reason, most devices have their own built-in memory, which are managed in a space distinct of the \cpu.


		\section{Distributed Memory}
		\tdi{get citation from COD}
		In a typical \smp system, all the processors share the same interface to connect to the main memory. These architectures are classified as \uma, given that all processors are seen as equal by the system and therefore all memory requests have the same latency. Such designs scale poorly with the number of processors, as one processor has to wait until all previous requests from other processors are fulfilled in order to access the main memory. Added to the gap between processor and memory speeds, this further aggravates the memory bottleneck.

		\numa architectures arise in response to the interface bottleneck. By moving the memory controller to the \cpu chip itself allows for each processor to have its own memory banks, thus parallelizing memory accesses. Yet, this causes memory accesses to take longer when the memory requested lies in another processor's memory, as the request has to go through the connection between the two processors. This increases the importance of controlling thread affinity, similar to what happens with memory cache. Some processors even implement \numa inside the chip. The Magny-Cours architecture from \amd, for example, consists of two Istambul dies in the same package, each being a \numa node with two memory channels \cite{AMD:MagnyCours}.

		In a single multiprocessor computational node, where multiple memories exist each directly connected to one processor, a single unified address space exists for all the main memory. This is called a shared memory system, where all the data lies in a single unified address space and every processor has access to all the memory implicitly. In other words, even if a given processor is not directly connected to the memory containing the required data, it can still address that data directly.

		A distributed memory system arises when two or more systems (including hardware accelerators), each with its own memory address space, cooperate in a computation. These systems communicate through message passing, having to explicitly transfer all the required data between the multiple address spaces. Communication becomes a possible bottleneck and, as such, extra effort is required to the workload distribution among the available resources in order to achieve the best computation to communication ratio.

		In most cases, \hetplats are distributed memory systems, each computational node having its own main memory and each hardware accelerator in those nodes having its own built-in memory. Between distinct computational nodes, communication is usually done over a network connection using an \mpi. When using an accelerator with its own memory, the solutions to implement communication depend on the tools used for development targeted for such devices.


		\section{Development Tools}
		Most developers use conventional sequential programming models, as this is the most natural way to plan the resolution of a given problem, one step at a time. For single-core systems, this worked perfectly, with the only parallelism in applications being extracted by the compiler and the processor at the instruction level. The transisiton to the multicore era brought together a new programming paradigm, which must be understood in order to fully take advantage of the most modern computing resources available.

		Making the transition to parallel programming is not trivial. The ability to concurrently run several execution threads exposes the programmer to new problems: data races, workload balancing, deadlocks, etc. Debugging parallel applications is also harder and it requires smarter approaches, better than simply tracing the code (anything with more than four threads will be very hard to keep track of). The problem becomes even more complex when trying to increase efficiency with \hetplats. Most of the times, a developer must be aware of the underlying architectural details in order to create an efficient implementation.

		Several tools have been presented, specially in the latest years, mas multicore shared memory environments and \hetplats become increasingly more popular. Frameworks have been developed to abstract the programmer from architectural details and the complexities of adapting code to run in a new platform.

		\subsection{OpenMP and TBB}
		\subsection{OpenMPC and OpenACC}
		\subsection{GAMA}
\end{document}
