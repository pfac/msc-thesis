\documentclass[../thesis]{subfiles}

\begin{document}
	\chapter{Technological Background}
	\label{chp:techbg}

	\tdg{parallelism in supercomputers}%
	Parallelism is far from being a modern concept, much less in the field of \hpc. Back in the 1970s, the raw performance of vector computers marked the beginning of modern Supercomputers \cite{Strohmaier:2005:20years}. These systems, able to process multiple items of data at the same time, prevailed in supercomputer design until the beginning of the 1990s, by which time the \mpp architectures became the most popular. \mpp systems had two or more identical processors connected to separate memory and controlled by separate (but identical) operating systems. Middle and low-end systems consisted in \smp architectures, containing two or more identical processors connected to a single shared main memory and controlled by a single operating system. In the following years cluster systems became the dominating design. In cluster computing multiple separate computers, each having a \smp architecture, are able to cooperate appearing as a single system to the user. This trend has continued up to the present \cite{TheNextWave:1:2013:Supercomputers,TOP500:overtime}.

	\tdg{classes for parallelism}%
	\citewithauthor{Hennessy:Patterson:2012:CAQA} define two classes of parallelism:
		\begin{description}
			\item [Data-Level Parallelism] consists in many data items being computed at the same time;
			\item [Task-Level Parallelism], where different tasks of work are able to operate independently and largely in parallel.
		\end{description}
	\tdg{parallelism in hardware} The hardware can exploit these two kinds of parallelism at four different levels. Processors take advantage of \ilp without any intervention from the programmer through mechanisms like superscalarity, out-of-order execution, pipelining and speculative execution. Vector processing on the other hand, uses a single instruction to operate over a collection of data in parallel, similar to what happened with vector computers but at a smaller scale. \tlp enables both data and task level parallelism by allowing more than one thread of instructions to be executed at the same time. Threads can also be used to hide memory latencies, by allowing another thread to use the physical resources while the memory request is not fulfilled. Lastly, \rlp, used in warehouse-scale computing, exploits parallelism among largely decoupled tasks specified by the programmer or the operating system.

	\tdg{software has to be redesigned}%
	The multicore advent is forcing a deep in change in software development. Legacy software, intrinsically sequential, is no longer able to profit from the evolution of computational hardware as new generations of microprocessors work at roughly the same speed (sometimes even less) but provide extra resources these applications are not prepared to take advantage of. This implies a re-design of old applications, otherwise they will plateau at or near current levels of performance, facing the risk of stagnation and loss of competitiveness, both for themselves and any projects that might depend on these legacy applications \cite{Farber:2011:RedefiningWhatIsPossible}.

		\section{Heterogeneous Platforms}
		As the evolution of microprocessors move towards higher levels of parallelism, several other options exist, from using multiple machines in a cluster, allowing each to perform part of the computation independently, to specific-purpose devices such as \dsps and \gpus.

		\tdg{definition of hetplats}%
		A given system is said to be heterogeneous when it contains multiple devices of different kinds. Usually, each of these devices is capable of computing several operations in parallel. The most efficient computing systems of the world in the TOP500 list\footnote{\url{http://www.top500.org}} are composed of several interconnected computing nodes, each with multiple multicore \cpus and one or more specialized hardware accelerators. \gpus and \intel\xeon Phi devices are currently the most popular.

		\tdg{GPUs and MICs (summary)}%
		Popularity of these new specialized devices in \hpc, some created and developed in completely separate environments, has been increasing in the last years, triggered by the trend to use the number of cores in computing hardware. \gpus evolution, for example, where throughput is more important than speed, led to massively parallel architectures, able to process hundreds of pixels at the same time. Devices based on the \intel\mic architecture, on the other hand, are placed between \gpus and \cpus, having the characteristics for massive parallelism while still being able to handle more complex operations (such as running an operating system). Both kinds of devices are explained in more depth in \cref{chp:cuda,chp:mic} where they are explored in the context of this document's case study.

		\tdg{alternative accelerators}%
		\dsps are another class of accelerators recently made popular for \hpc due to new architectures able to perform floating-point computations. Their architecture is very similar to that of a conventional processor, both programming and memory-wise \cite{FLAWN61}. Alternatively, \fpgas mix a set of configurable logic blocks, digital signal processor blocks and traditional \cpu cores (optional), all using a configurable interconnect. The key characteristic of these devices is the ability to configure them for a specific type of computation making them extremely versatile \cite{Brodtkorb:2010}. These devices are described here for completeness, not being the scope of this document explore the case study using them.

		\tdg{processor-memory gap in accelerators}%
		Most hardware accelerators suffer from the same limitations as conventional processor architectures regarding memory latency (explained in \cref{sec:techbg:distmem}), despite the many strategies each one implements to hide or overcome the problem. Also, since the connection between the \cpu and an accelerator is typically performed over a \pcie interface, using the same memory banks would be a major bottleneck. For this reason, most devices have their own built-in memory hierarchy, which is managed in a space distinct of the \cpus.


		\section{Distributed Memory}
		\label{sec:techbg:distmem}
		\tdi{get citation from COD}

		\tdg{processor-memory gap}%
		\cpus and memories evolution followed very distinct paths. While the former focused on speed, the latter focused on capacity. Throughout the decades, this created and aggravated a performance gap between the processor and the memory, with memory accesses taking much longer than instruction execution (around 100 times more). In \cpus, this limitation was overcome with the creation of a memory hierarchy, with the large main memory in the bottom, and multiple levels of cache memory, each smaller, faster and closer to the computing cores.

		\tdg{uniform memory accesses}%
		In a typical \smp system, all the processors share the same interface to connect to the main memory. These architectures are classified as \uma, given that all processors are seen as equal by the system and therefore all memory requests have the same latency. Such designs scale poorly with the number of processors, as one processor has to wait until all previous requests from other processors are fulfilled in order to access the main memory. Added to the gap between processor and memory speeds, this further aggravates the memory bottleneck.

		\tdg{non-uniform memory accesses}%
		\numa architectures arise in response to the interface bottleneck. By moving the memory controller to the \cpu chip itself allows for each processor to have its own memory banks, thus parallelizing memory accesses. Yet, this causes memory accesses to take longer when the memory requested lies in another processor's memory, as the request has to go through the connection between the two processors. This increases the importance of controlling thread affinity, similar to what happens with memory cache. Some processors even implement \numa inside the chip. The Magny-Cours architecture from \amd, for example, consists of two Istambul dies in the same package, each being a \numa node with two memory channels \cite{AMD:MagnyCours}.

		\tdg{shared memory}%
		In a single multiprocessor computational node, where multiple memories exist each directly connected to one \cpu, a single unified address space exists for all the main memory. This is called a shared memory system, where all the data lies in a single unified address space and every processor has access to all the memory implicitly. In other words, even if a given processor is not directly connected to the memory containing the required data, it can still address that data directly.

		\tdg{shared memory NUMA}%
		Implementing a shared-memory \numa architectures introduces the complexity of maintaining the cache of multiple processors coherent. This is required in order for the multiple processors to be able to use the same data. When one processor changes a shared data item, the coherency mechanism notifies the remaining processors that the copy in their private cache is outdated and the item must be reloaded from main memory. Maintaining coherency guarantees the correction of the program, but it hampers the scalability of \numa architectures.

		\tdg{distributed memory}%
		A distributed memory system arises in \hetplats, where each computational node has its own main memory address space. A single computational node may also implement a distributed memory architecture if it contains any hardware accelerator with its own built-in memory (with its own address space). These systems communicate through message passing, having to explicitly transfer all the required data between the multiple address spaces. Between distinct computational nodes, communication is usually done over a network connection using an \mpi library. In the case of accelerators, the solutions to implement communication depend on the tools used for development targeted for such devices. Communication becomes a possible bottleneck with distributed memory. As such, extra effort is required to the distribute the workload efficiently among the available resources in order to achieve the best computation to communication ratio.

		\section{Development Tools}
		\tdg{new programming paradigm}%
		Most developers use conventional sequential programming models, as this is the most natural way to plan the resolution of a given problem, one step at a time. For single-core systems, this worked perfectly, with the only parallelism in applications being extracted by the compiler and the processor at the instruction level. The transition to the multicore era brought together a new programming paradigm, which must be understood in order to fully take advantage of the most modern computing resources available.

		\tdg{parallel programming requires new tools, methods and deeper arch knowledge}%
		Making the transition to parallel programming is not trivial. The ability to concurrently run several execution threads exposes the programmer to new problems: data races, workload balancing, deadlocks, etc. Debugging parallel applications is also harder and it requires smarter approaches, better than simply tracing the code (anything with more than four threads will be very hard to keep track of). The problem becomes even more complex when trying to increase efficiency with \hetplats. Most of the times, a developer must be aware of the underlying architectural details in order to create an efficient implementation.

		\tdg{development tools}%
		Several tools have been presented to aid developers in taking advantage of the resources available in multicore shared memory environments and \hetplats, specially in the latest years with the increasing popularity of such systems. Despite none being explored in the scope of this dissertation, many frameworks have also been developed to abstract the programmer from architectural details and the complexities of adapting code to run in a new platform (like a hardware accelerator).

		\subsection{PThreads, OpenMP, TBB and Cilk}
		\pthreads names the standardized C language threads programming interface for UNIX systems. This standard was introduced with the goal of making parallel programming in shared-memory systems portable when hardware vendors implemented their own proprietary versions of threads \cite{LLNL:Barney:pthreads}. This API provides the tools for managing threads, mutual exclusion, condition variables, read/write locks and per-thread context.

		OpenMP\cite{OpenMP,OpenMP:spec3.1} was formulated under a need similar to the purpose of \pthreads: to abstract the different ways operating systems imposed for programming for threads. At the time (1997), UNIX used \pthreads, Sun used Solaris threads, Windows used its own API and Linux used Linux threads \cite{Intel:pthreads_or_openmp}.

		OpenMP is a standard API, in C/C++ and Fortran, for parallel programming in a multi-platform shared memory application running on all architectures. The API itself is less verbose than \pthreads and is very simple and easy to use (most of the times through compiler directives). It abstracts an inexperience programmer from all the complexity of managing threads, but without lacking the required tools for advanced users to perform any kind of fine tuning. It is also portable and scalable. OpenMP only addresses homogeneous systems with conventional \cpus and schedules automatically efficient workload distributions amongst all the available resources.

		\intel\tbb is a C++ template library created by \intel with a similar purpose to OpenMP. While it is a lot more verbose, and lacks support for other languages, \tbb provides algorithms, highly concurrent containers, locks and atomic operations, a task scheduler and a scalable memory allocator \cite{TBB}. It is harder to program than OpenMP, but \intel claims it achieves equivalent or better performance.

		\subsection{OpenMPC and OpenACC}

		OpenMPC\cite{OpenMPC} consists in an extension of the OpenMP specification to provide translation from regular OpenMP compiler directives to \cuda code.

		Parallel zone directives delimit the blocks of code candidate for \cuda kernels. Only loop and section directives are considered true parallel sections, which are translated to perform workload distribution amongst the available threads. Synchronization directives cause the kernels to be split, as this is the only way to force global synchronization amongst all threads. Directives specifying data properties are interpreted to find the best \gpu memory space for the required data.

		OpenACC\cite{OpenACC:2.0} is a standard API, in the same languages as OpenMP, meant to bring the advantages of OpenMP to programming with hardware accelerators. While originally designed only for \gpus, support has been extended for the \intel\xeonphi coprocessor. It abstracts the programmer from the memory management, kernel creation and the accelerator management. It also allows to execute both on the device and the \gpu host at the same time.

		Comparing, OpenMPC only provides support only for \cuda-enabled devices, while OpenACC supports \nvidia and \amd\gpus alike and \intel\mic devices.
\end{document}
