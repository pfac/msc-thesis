\documentclass[../thesis]{subfiles}

\begin{document}
	\chapter{Technological Background}
	\label{chp:techbg}

	\tdg{parallelism in supercomputers}%
	Parallelism is far from being a modern concept, much less in the field of \hpc. Back in the 1970s, the raw performance of vector computers marked the beginning of modern Supercomputers \cite{Strohmaier:2005:20years}. These systems, able to process multiple items of data at the same time, prevailed in supercomputer design until the beginning of the 1990s, by which time the \mpp architectures became the most popular. \mpp systems had two or more identical processors connected to separate memory and controlled by separate (but identical) operating systems. Middle and low-end systems consisted in \smp architectures, containing two or more identical processors connected to a single shared main memory and controlled by a single operating system. In the following years cluster systems became the dominating design. In cluster computing multiple separate computers, each having a \smp architecture, are able to cooperate appearing as a single system to the user. This trend has continued up to the present \cite{TheNextWave:1:2013:Supercomputers,TOP500:overtime}.

	\tdg{classes for parallelism}%
	\citewithauthor{Hennessy:Patterson:2012:CAQA} define two classes of parallelism:
		\begin{description}
			\item [Data-Level Parallelism] consists in many data items being computed at the same time;
			\item [Task-Level Parallelism] refers to different tasks of work able to operate independently and largely in parallel.
		\end{description}
	\tdg{parallelism in hardware} The hardware can exploit these two kinds of parallelism at four different levels. Processors take advantage of \ilp without any intervention from the programmer through mechanisms like superscalarity, out-of-order execution, pipelining and speculative execution. Vector processing on the other hand, uses a single instruction to operate over a collection of data in parallel, similar to what happened with vector computers but at a smaller scale. \tlp enables both data and task level parallelism by allowing more than one thread of instructions to be executed at the same time. Threads can also be used to hide memory latencies, by allowing another thread to use the physical resources while the memory request is not fulfilled. Lastly, \rlp, used in warehouse-scale computing, exploits parallelism among largely decoupled tasks specified by the programmer or the operating system.

	\tdg{software has to be redesigned}%
	For decades programmers did not have to worry with exploring parallelism in their applications but the multicore advent is forcing a deep change in software development. Legacy software, intrinsically sequential, is no longer able to profit from the evolution of computational hardware as new generations of microprocessors work at roughly the same clock frequency (sometimes even less), instead providing extra resources these applications are not prepared to take advantage of. This implies a re-design of old applications, otherwise they will plateau at or near current levels of performance, facing the risk of stagnation and loss of competitiveness, both for themselves and any projects that might depend on these legacy applications \cite{Farber:2011:RedefiningWhatIsPossible}.

	\subfile{tex/210.techbg.hetplats.tex}


		\section{Distributed Memory}
		\label{sec:techbg:distmem}
		\tdi{get citation from COD}

		\tdg{processor-memory gap}%
		\cpus and memories evolution followed very distinct paths. While the former focused on speed, the latter focused on capacity. Throughout the decades, this created and aggravated a performance gap between the processor and the memory, with memory accesses taking much longer than instruction execution (around 100 times more). In \cpus, this limitation was overcome with the creation of a memory hierarchy, with the large main memory in the bottom, and multiple levels of cache memory, each smaller, faster and closer to the computing cores.

		\tdg{uniform memory accesses}%
		In a typical \smp system, all the processors share the same interface to connect to the main memory. These architectures are classified as \uma, given that all processors are seen as equal by the system and therefore all memory requests have the same latency. Such designs scale poorly with the number of processors, as one processor has to wait until all previous requests from other processors are fulfilled in order to access the main memory. Added to the gap between processor and memory speeds, this further aggravates the memory bottleneck.

		\tdg{non-uniform memory accesses}%
		\numa architectures arise in response to the interface bottleneck. By moving the memory controller to the \cpu chip itself allows for each processor to have its own memory banks, thus parallelizing memory accesses. Yet, this causes memory accesses to take longer when the memory requested lies in another processor's memory, as the request has to go through the connection between the two processors. This increases the importance of controlling thread affinity, similar to what happens with memory cache. Some processors even implement \numa inside the chip. The Magny-Cours architecture from \amd, for example, consists of two Istambul dies in the same package, each being a \numa node with two memory channels \cite{AMD:MagnyCours}.

		\tdg{shared memory}%
		In a single multiprocessor computational node, where multiple memories exist each directly connected to one \cpu, a single unified address space exists for all the main memory. This is called a shared memory system, where all the data lies in a single unified address space and every processor has access to all the memory implicitly. In other words, even if a given processor is not directly connected to the memory containing the required data, it can still address that data directly.

		\tdg{shared memory NUMA}%
		Implementing a shared-memory \numa architectures introduces the complexity of maintaining the cache of multiple processors coherent. This is required in order for the multiple processors to be able to use the same data. When one processor changes a shared data item, the coherency mechanism notifies the remaining processors that the copy in their private cache is outdated and the item must be reloaded from main memory. Maintaining coherency guarantees the correction of the program, but it hampers the scalability of \numa architectures.

		\tdg{distributed memory}%
		A distributed memory system arises in \hetplats, where each computational node has its own main memory address space. A single computational node may also implement a distributed memory architecture if it contains any hardware accelerator with its own built-in memory (with its own address space). These systems communicate through message passing, having to explicitly transfer all the required data between the multiple address spaces. Between distinct computational nodes, communication is usually done over a network connection using an \mpi library. In the case of accelerators, the solutions to implement communication depend on the tools used for development targeted for such devices. Communication becomes a possible bottleneck with distributed memory. As such, extra effort is required to the distribute the workload efficiently among the available resources in order to achieve the best computation to communication ratio.

		\subfile{tex/230.techbg.tools.tex}
	\end{document}
