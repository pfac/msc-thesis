%!TEX root = ../thesis.tex
\documentclass[../thesis]{subfiles}

\begin{document}
	\section{Analysis}
	\label{sec:multicore:analysis}

	The results presented in this chapter confirm those described in \cite{Deadman:Higham:Ralha:2013}. The explicit parallelism made available by the diagonal strategy easily overcomes the lack of locality when compared with the colum/row strategy, and both methods scale very well, reaching the peak performance when all the available resources are used.

	Using the block method is also clearly more efficient than using the point method, although there is less speedup from blocking as the number of threads is increased. Still, the accumulated speedup shows a clear ``bathtub'' curve, with the peak performance being reached when using all the hardware supported threads in the \cpu.

	The sensitivity of the point method is specially strange. There is a particular case, with the matrix dimension of $n = 2048$, where the execution time of the point method is the double of $n = 2047$ or $n = 2049$. Such strange behaviour does not happen with the block method, which implies that something happens at cache level with this particular size. In fact, research shows similar cases where the authors call this effect ``cache resonance'' \cite{MathWorks:LAPACK,SO:cache_resonance}. Basically, what happens is that this particular size causes the access stride to reach only a small group of cache lines. This rapidly saturates the cache ways available, causing capacity misses.

	The scalability of the algorithm shown in these results increase the expectations of using the massive parallelism made available by the hardware accelerators studied in this dissertation.

\end{document}
