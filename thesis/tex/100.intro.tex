\documentclass[../thesis]{subfiles}

\begin{document}
	\chapter{Introduction}
	\label{chp:intro}

	\tdg{multicore evolution through frequency and transistors}%
	For more than half a century, computational systems have evolved at an increasing rate, fueled by a similar demand in computing power. The invention of the digital transistor, in 1947, smaller and faster than its predecessor, made computers smaller and more power efficient. Integrated circuits further reduced the space and power requirements of computers, which in turn led to the emergence of the microprocessor. The evolution of the microprocessor in the following decades followed had two main contributors: the number of transistors and their clock frequency.

	\tdg{frequency goes faster, even with old software}%
	A processor's clock frequency is strongly correlated to the rate at which it can issue instructions, which means that a higher frequency is roughly translated into more instructions being processed each second. This meant that the same old applications got faster just with the evolution of the underlying hardware, without any programming effort. On the other hand, increasing the number of transistors in a chip allowed to create complex systems that optimized software execution, mainly with the exploration of \ilp and the use of memory caches to hide the increasing memory latency times.

	\tdg{Moore's Law}%
	In 1965, Gordon Moore, in an attempt to predict the evolution of integrated circuits during the following decade, projected that the number of transistors would double every year \cite{Moore:1965}. In 1975, adding more recent data, he slowed the future rate of increase in complexity to what is still today known as Moore's Law: the number of transistors in a chip doubles every two years \cite{Moore:1975,ComputerHistory:Moore}.

	\tdg{frequency reached the limits, multicore is the future}%
	In 2003, the evolution of the microprocessor reached a milestone. Until then, the increase in clock frequency in microprocessors had followed closely the number of transistors, but the power density in processors was approaching the physical limitations of silicon-based microelectronics with air cooling techniques. Continuing to increase the frequency would require new and prohibitively expensive cooling solutions. With Moore's Law still in effect, and the lack of more \ilp to explore efficiently, the key \cpu chip designers turned in a new direction, replacing old single heavy core \cpus with multiple simpler cores working in parallel and sharing common resources.

	\tdg{new programming paradigm and heterogeneous platforms}%
	The advent of multicore architectures also shifted the software development trends, since sequential code is no longer able to efficiently use these multiple computing resources, available in modern systems. It also boosted the popularity of special purpose devices, like \gpus and \dsps. Besides having particular features that might be suited for certain problems, using these devices to perform part of the computation allows the \cpu to be focused on other tasks, or to be focused in only a part of the domain, thus increasing efficiency. This can be further extended by interconnecting several computational nodes, each with one or more \cpus and some specialized devices, able to cooperate over a fast network interface. Such systems are called \hetplats.

	\tdg{NAG}%
	The \nag\cite{NAG} delivers a highly reliable commercial numerical library containing several specialized multicore functions for matrix operations. While the \nag library includes implementations of several algorithms for CUDA-enabled \gpus and the \intel\xeonphi coprocessor in heterogeneous platforms, it has yet no matrix square root function optimized for these devices \cite{NAG:GPU:0:6,NAG:MIC}.

	\tdg{MAGMA and lack of alternative implementations}%
	\magma is a project that aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current CPU+GPU systems. At the moment, \magma already includes implementations for many of the most important algorithms in Matrix Algebra but not for the computation of the square root \cite{PLASMA:MAGMA}. This feature is also not implemented in any of the major GPU accelerated libraries listed by \nvidia \cite{ACCELEREYES:WIKI:SQRTM,CULA:LAPACK,NVIDIA:CUBLAS:5:0,NVIDIA:CUSPARSE:5:0,CUSP:FEATURES}.

	\tdg{Previous work: blocked SQRTM}%
	In a previous work, \citewithauthor{Deadman:Higham:Ralha:2013} expanded the method devised by \citewithauthor{bjorck:hammarling:1983} to compute the square root of a matrix, implementing an equivalent blocked method in a multicore environment. While blocked approaches are able to make a more efficient use of the memory hierarchy, they are also very well suited for devices designed for vector processing, such as \acfp{GPU} and devices based on \intel\mic architecture.

	\subfile{tex/110.intro.goals.tex}
	\subfile{tex/120.intro.organization.tex}
\end{document}
