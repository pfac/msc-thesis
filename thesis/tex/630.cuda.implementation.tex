\documentclass[../thesis]{subfiles}

\begin{document}
	\section{Implementation}
	\label{sec:cuda:implementation}
	
	\tdg{Can't use multicore or MIC code}%
	Unlike \mic devices, \gpus differ greatly from \cpus. The distinct programming model for this kind of devices requires a shift in the way the programmer thinks about the algorithm. Consequently, little of the code implemented in \cref{chp:multicore,chp:mic} is reusable in a CUDA implementation of the matrix square root algorithm.

	\tdg{NVCC is incompatible with Armadillo}%
	First experiments have shown that the \nvidia compiler is incompatible with recent versions of the GNU compiler, which prevents the usage of modern features in the C++ language used by the Armadillo library. Although the usage of this library was reduced to loading the matrix file and outputting the result in \cref{sec:mic:optims}, and the incompatibility was isolated and found not to be related with these input/output operations, the Armadillo library is prepared to have all its headers used simultaneously. This very tight coupling results in having to remove any trace of the library from the CUDA implementation.

	\tdg{Removing Armadillo meant reimplementing I/O}%
	Removing Armadillo implied that the code to load the matrix files had to be ported to a compatible implementation. To ease the task, \texttt{ARMA\_ASCII} was selected as the default format. This is the simplest text format in Armadillo, with the files having a small header (meant to identify the data type and the dimensions of the matrix) immediately followed by the matrix content.

	\tdg{No optimized BLAS available for block scope}%
	Contrary to the solutions in the previous chapters, a \cuda implementation of this algorithm can not take advantage of optimized \blas and \lapack libraries. The available packages assume that its kernels will have the entire device available, and most \lapack packages do not even implement \texttt{TRSYL}. Experience from \cref{chp:mic} show that this is not the case since both methods contain independent parallel calls to \blas and \lapack functions. This implies having to reimplement each of these functions so that they can be used by all the threads in a single \cuda block.

	\tdg{Each diagonal function is a different kernel}%
	Synchronization is also different for this implementation. In previous chapters, the parallel zones were confined to the computation of each diagonal, which were iterated over sequentially. This introduced the synchronization necessary to prevent that any diagonal were computed without its dependences being ready. In the context of a \cuda kernel, it is not possible to synchronize the entire device, consequence of blocks having to be independent. Therefore, the only way to implement this synchronization (for both methods) is to have each diagonal computed by a different kernel, at the expense of having to wait for the kernel to return from the device before launching a new one.

	\tdg{OpenMPC's approach}
	Kernels were implemented following an approach similar to OpenMPC by translating the regions of parallel execution (delimited by OpenMP \texttt{for} directives) in \cref{chp:multicore,chp:mic}. This allowed for the improvements described in \cref{sec:mic:optims} to affect how the kernels were implemented.

	\tdg{Point method}%
	Since only one diagonal of elements can be computed in parallel at any time, the point method is implemented using one-dimensional grid and blocks. This is the simplest method since linearising the indices, from both the threads and the blocks, each element in the diagonal can be computed by one thread. There is a trade-off: using one thread per element allows to take advantage of the larger parallelism among elements in the beginning, but suffers from lack of parallelism after some point since it does not concurrently solve the dependencies. On the other hand, using a whole block per element would hamper performance in the beginning and improve as the algorithm advances since it would be able to solve the dependencies in parallel. These implementations are named coarse point-diagonal (cPD) and fine point-diagonal (fPD) in \cref{sec:cuda:results}.

	\tdg{Block method}%
	With the block method this trade-off disappears and gives place to the lack of optimized \blas and \lapack libraries. It also introduces the possibility for using two-dimensional blocks since each block works as a standalone matrix, yet the kernels are kept with one-dimensional blocks for compatibility with the point method. Blocks in the main diagonal are solved using a single-block implementation of the point method. For the remaining diagonals, the entire thread block computes the indices (effectively isolating the required blocks for the computation) and calls the implemented single-block \blas and \lapack functions.
\end{document}
