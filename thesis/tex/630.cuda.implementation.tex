\documentclass[../thesis]{subfiles}

\begin{document}
	\section{Implementation}
	\label{sec:cuda:implementation}
	
	\tdg{Can't use multicore or MIC code}%
	Unlike \mic devices, \gpus differ greatly from \cpus. As such, the distinct programming model for this kind of devices requires a shift in the way the programmer thinks about the algorithm. Consequently, little of the code implemented in \cref{chp:multicore,chp:mic} is reusable in a CUDA implementation of the matrix square root algorithm.

	\tdg{NVCC is incompatible with Armadillo}%
	First of all, the \nvidia compiler is incompatible with recent versions of the GNU compiler, which prevents the usage of modern features in the C++ language used by the Armadillo library. Although the usage of this library was reduced to loading the matrix file and outputting the result in \cref{sec:mic:optims}, and the incompatibility was isolated and found not to be related with these input/output operations, the Armadillo library is prepared to have all its headers used simultaneously. This very tight coupling results in having to remove any trace of the library from the CUDA implementation.

	\tdg{Removing Armadillo meant reimplementing I/O}%
	Removing Armadillo implied that the code to load the matrix files had to be ported to a compatible implementation. To ease the task, \texttt{ARMA\_ASCII} format was selected as the reference format. This is the simplest text format in Armadillo, with the files having a small header (meant to identify the data type and the dimensions of the matrix) immediately followed by the matrix content.

	\tdg{No optimized BLAS available for block scope}%
	Contrary to the implementations in the previous chapters, a \cuda implementation of this algorithm can not take advantage of optimized \blas and \lapack libraries. The available packages assume that its kernels will have the entire device available, and most \lapack packages do not even implement \texttt{TRSYL}. Experience from \cref{chp:mic} show that this is not the case since both methods contain independent parallel calls to \blas and \lapack functions. This implies having to reimplement each of these functions so that they can be used by all the threads in a single \cuda block.

	\tdg{Each diagonal function is a different kernel}%
	Synchronization is also different for this implementation. In previous chapters, the parallel zones were confined to the computation of each diagonal, which were iterated over sequentially. This introduced the synchronization necessary to prevent that any diagonal were computed without its dependences being ready. In the context of a \cuda kernel, it is not possible to synchronize the entire device, consequence of blocks having to be independent. Consequently, the only way to implement this synchronization (for both methods) is to have each diagonal computed by a different kernel, at the expense of having to wait for the kernel to return from the device before launching a new one.

	\tdg{Point method}%
	Since only one diagonal of elements can be computed in parallel at any time, the point method is implemented using one-dimensional grid and blocks. This is the simplest method since linearising the indices, from both the threads and the blocks, each element in the diagonal can be computed by one thread. There is a trade-off: using one thread per element allows to take advantage of the larger parallelism amongst elements in the beginning, but suffers from lack of parallelism after some point since it does not solve the dependencies concurrently. On the other hand, using a whole block per element would hamper performance in the beginning but it would improve as the algorithm advances since it would be able to solve the dependencies in parallel.

	\tdg{Block method}%
	With the block method this trade-off disappears and gives place to the lack of optimized \blas and \lapack libraries. It also introduces the possibility for using two dimensional blocks since each block works as a standalone matrix, yet the kernels are kept with one-dimensional blocks for compatibility with the point method. Blocks in the main diagonal are solved using a single-block implementation of the point method. For the remaining diagonals, the entire thread block computes the indices (effectively isolating the required blocks for the computation) and calls the implemented single-block \blas and \lapack functions.
\end{document}
