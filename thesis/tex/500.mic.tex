\documentclass[../thesis]{subfiles}

\begin{document}
	\chapter{Intel MIC}
	\label{chp:mic}
	\section{Architecture}
	The \intel\xeon Phi Coprocessor contains up to 61 fully functional in-order \intel\mic Architecture cores running at 1GHz (up to 1.3GHz), each containing 64KB of L1 cache (evenly split for data and instructions) and 512KB of L2 cache.

	The device can support 8 memory controllers with two \gddr5 channels each. These support a total of 5.5GT/s, corresponding to a theoretical aggregate bandwidth of 352 GB/s. At the time, the maximum memory size available is 16GB\footnote{\intel\xeonphi Coprocessor 7120X\cite{datasheet:XeonPhi:7120X}}.

	A high performance on-die bidirectional  ring connects the L2 caches, the memory controllers and the \pcie interface logic. The connected L2 caches allows for requests to be fulfilled from other cores' L2 cache faster than it would be from memory, thus implementing a last-level cache with over 30MB. Cache coherency is preserved across the entire coprocessor through a distributed tag directory using a reversible one-to-one hashing function.

	\intel\mic Architecture is based on the x86 \isa, extended with 64-bit addressing and 512-bit wide \simd vector instructions and registers. Yet, it does not support other \simd\isas (\mmx, \intel\sse and \intel\avx).

	Each coprocessor core supports up to 4 hardware threads and can execute 2 instructions per clock cycle, one on the U-pipe and one on the V-pipe. Each hardware thread has a ``ready-to-run'' buffer comprising two instruction bundles, each bundle representing two instructions that can be executed simultaneously.

	The \vpu contains 32 vector registers, each 512-bit wide. It includes the \emu and is able to execute up to 16 integer or single-precision floating-point operations per cycle (half for double-precision). Additionally, each operation can be a floating-point multiply-add, thus doubling the number of operations in each cycle. Fully utilizing the \vpu is critical for achieving high performance with the coprocessor.

	\section{Execution modes}
	\intel\mic devices have a Micro Operating System, a Linux\textsuperscript{*}-based operating system, contrary to what happens with most accelerator devices. This enables the coprocessors to work as a separate remote network node, independent of the host system.

	These devices are able to operate in three different modes:
		\begin{description}
			\item [Native] The application is run solely on the coprocessor, as if it were a remote network node;
			\item [Offload] The host system offloads work to the coprocessor, as is usually done when using hardware accelerators;
			\item [Message Passing] Using \mpi, the coprocessor is treated as another peer in the network.
		\end{description}

	The native mode is only one that allows all the cores to be used, since it is not necessary for the OS to be managing the system, something that requires one of the cores to be exclusive in other modes. Running an application natively in the coprocessor requires that it is specifically built for its architecture, which in the \icc is done by providing the \texttt{-mmic} flag to both in the compiling and linking stages.

	Native applications also require libraries specifically built for the \intel\mic architecture. While the \intel libraries are made available by default in the \intel Composer XE Suites, third-party libraries like Boost have to be specially built for this architecture. These libraries are then required in the linking phase of the building process and while running the application. This implies that these libraries must be copied to the device together with the application. 

	\tdi{Message passing mode}

	Offload mode treats the device as a typical hardware accelerator, similar to what happens with a \gpu, using compiler directives (\texttt{pragma offload} in C/C++) to control the application behaviour. Code for both the host and the coprocessor are compiled in the host environment. During the execution of the first offloaded code, the runtime loads the executable and the libraries linked with the code onto the coprocessor, and these remain on the device memory until the host program terminates (thus maintaining state across offload instances).

	The offload code regions may not run on the coprocessor, depending on whether any device is present and it has any resources available. In these cases the offload code regions are executed on the host.

	As happens with other hardware accelerators, offloading work to the device requires moving data between the host and the coprocessor. Using the offload directive this is done explicitly as directive clauses. An \texttt{in} clause defines the data that must be transferred from the host to the device before executing the offload region. \texttt{out} transfers the data from the device to the host at end of the offloaded code. Additionally, \texttt{inout} merges the functionality of both clauses, avoiding clause duplication. Using this memory copy model the data must be scalar or bitwise copyable structs/classes, i.e., arrays or structs/classes containing pointers are not supported. Exceptionally, variables used within the offload construct but declared outside its scope are synchronized automatically before and after execution in the coprocessor.

	Alternatively, data can be transferred to the device implicitly using two new \intel\cilk Plus keywords: \texttt{\_Cilk\_shared} and \texttt{\_Cilk\_offload}. The former is used to declare a variable ``shared'' between the host and the coprocessor. This data is synchronized at the beginning and end of offload functions marked with the \texttt{\_Cilk\_offload} keyword. This implicit memory copy model surpasses the limitations of the explicit model in the offload directive, allowing for complex, pointer-based data structures.

	\section{Programming model}
	\section{MKL}
	\tdi{MKL automatic offload and compiler assisted offload}

	\section{Native execution}
	The native execution mode provides several advantages over its alternatives. To start, it makes one extra core available. Given that the coprocessor's cores architecture is based on the x86 \isa, it also reduces the development time considerably, as a \cpu functional implementation requires only to be rebuilt targeting the \mic architecture in order for the device to be able to run it natively. It also skips the communication necessary in an offload-based implementation, which is a potential bottleneck for many applications.

	For these reasons, this mode was selected as a first attempt to use the \intel\xeonphi Coprocessor.

	As previously stated, no change was required to the code developed in the previous chapter, the only change being in the build process (the \texttt{-mmic} flag). Yet, the previous build system was not prepared for the \intel\xeonphi Coprocessor, since it implied cross-compilation, and had to be adapted.

	\subsection{Results}
		\tdi{Show the disgrace that were these results compared to the multicore implementation}

	\section{Optimizations}
	\label{sec:mic:optims}
		The results presented in the previous section were surprising, given the successful results obtained with the multicore implementation in \cref{chp:multicore} and the resources available in the \intel\xeonphi Coprocessor. The discrepancy is so big that a decision was made at this point to improve the performance of this implementation before exploring any other execution modes or programming models.

		\subsection{Loop Unrolling}
		Revisiting \cref{chp:case,alg:multicore:diagonal:point,alg:multicore:diagonal:block}, in particular the description of the algorithm dependencies, a deeper analysis allows to conclude that it is logical to unroll the diagonal loop. In both algorithms, the first and second diagonals act differently from the rest. The first diagonal has no dependencies and, as such, recursively applies the square root on the focused element/block (standard \texttt{sqrt} in the point method, which in turn is used by the block method).

		On the other hand, the elements/blocks in the second diagonal depend only of those in the main diagonal. As such, there are no dependencies to solve, as the main diagonal elements/blocks are used directly to compute the final result.

		The following diagonals perform additional work, having to compute how the elements/blocks on the left and below affect the input value, where this affected value is the one used to compute the final result.

		\begin{algorithm}[htp]
			\caption[Matrix Square Root Unrolled (diagonal, point)]{Matrix Square Root (diagonal, point)}
			\label{alg:mic:diagonal:point}
			\DontPrintSemicolon

			\SetKwFunction{maind}{sqrtm\_d0}
			\SetKwFunction{firstd}{sqrtm\_d1}
			\SetKwFunction{otherd}{sqrtm\_dn}

			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;
			fill $U$ with zeros\;

			\maind{$T$, $U$}\;
			\firstd{$T$, $U$}\;
			\For{$d \leftarrow 2$ \KwTo $n-1$}{
				\otherd{$d$, $T$, $U$}\;
			}
		\end{algorithm}

		\Cref{alg:mic:diagonal:point} shows the unrolled algorithm for the point method, using three distinct functions, one for each case. \texttt{sqrtm\_d0} handles the main diagonal ($d = 0$), \texttt{sqrtm\_d1} handles the first super-diagonal ($d = 1$) and \texttt{sqrtm\_dn} handles all the other diagonals, ($d$ is provided as an argument in the function call). These functions are described in \cref{alg:mic:diagonal:point:0,alg:mic:diagonal:point:1,alg:mic:diagonal:point:n}, respectively. \Cref{alg:mic:diagonal:block:0,alg:mic:diagonal:block:1,alg:mic:diagonal:block:n} show the corresponding algorithms for the block method, following the same index expansion logic described in \cref{chp:multicore}.

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- main diagonal (point)}
			\label{alg:mic:diagonal:point:0}
			\DontPrintSemicolon

			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;

			\For{$e \leftarrow 0$ \KwTo $n-1$}{
				$U_{ee} \leftarrow \sqrt{T_{ee}}$\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- first super-diagonal (point)}
			\label{alg:mic:diagonal:point:1}
			\DontPrintSemicolon

			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;

			\For{$e \leftarrow 0$ \KwTo $n-2$}{
				$i \leftarrow e$\;
				$j \leftarrow e + 1$\;
				$U_{ij} \leftarrow \frac{T_{ij}}{U_{ii}U_{jj}}$\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- other super-diagonals (point)}
			\label{alg:mic:diagonal:point:n}
			\DontPrintSemicolon

			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{The diagonal index $d$}
			\Input{A real upper triangular matrix $T$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;

			\For{$e \leftarrow 0$ \KwTo $n-d-1$}{
				$i \leftarrow e$\;
				$j \leftarrow e + d$\;
				$r \leftarrow$ sub-row in $i$ from $i+1$ to $j-1$\;
				$c \leftarrow$ sub-column in $j$ from $i+1$ to $j-1$\;
				$s \leftarrow r \times c$\;
				$U_{ij} \leftarrow \frac{T_{ij} - s}{U_{ii} \cdot U_{jj}}$\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption[Matrix Square Root Unrolled (diagonal, block)]{Matrix Square Root (diagonal, block)}
			\label{alg:mic:diagonal:block}
			\DontPrintSemicolon

			\SetKwFunction{maind}{sqrtm\_d0}
			\SetKwFunction{sqrtm}{sqrtm\_d1}
			\SetKwFunction{sqrtm}{sqrtm\_dn}
			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Input{The dimension of a full block $b$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;
			$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;
			fill $U$ with zeros\;

			\maind{$T$, $\#\mathrm{blocks}$, $b$, $U$}\;
			\firstd{$T$, $\#\mathrm{blocks} - 1$, $b$, $U$}\;
			\For{$d \leftarrow 2$ \KwTo $\#\mathrm{blocks}-1$}{
				\otherd{$d$, $T$, $\#\mathrm{blocks} - d$, $b$, $U$}\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- main diagonal (block)}
			\label{alg:mic:diagonal:block:0}
			\DontPrintSemicolon

			\SetKwFunction{min}{min}
			\SetKwFunction{range}{range}
			\SetKwFunction{sqrtm}{sqrtm}
			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
			\Input{The dimension of a full block $b$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;

			\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
				$i_0 \leftarrow e \cdot b$\;
				$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
				$i \leftarrow $\range{$i_0$, $i_1$}\;
				$U_{ii} \leftarrow $\sqrtm{${T_{ii}}$}\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- first super-diagonal (block)}
			\label{alg:mic:diagonal:block:1}
			\DontPrintSemicolon

			\SetKwFunction{min}{min}
			\SetKwFunction{range}{range}
			\SetKwFunction{sylvester}{sylvester}
			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
			\Input{The dimension of a full block $b$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;
			$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;

			\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
				$i_0 \leftarrow e \cdot b$\;
				$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
				$i \leftarrow $\range{$i_0$, $i_1$}\;
				$j_0 \leftarrow (e + 1) \cdot b$\;
				$j_1 \leftarrow $\min{$(e + 2) \cdot b$, $n$}$-1$\;
				$j \leftarrow $\range{$j_0$, $j_1$}\;
				$U_{ij} \leftarrow $\sylvester{$U_{ii}$, $U_{jj}$, $T_{ij}$}\;
			}
		\end{algorithm}

		\begin{algorithm}[htp]
			\caption{Matrix Square Root -- other super-diagonals (block)}
			\label{alg:mic:diagonal:block:n}
			\DontPrintSemicolon

			\SetKwFunction{min}{min}
			\SetKwFunction{range}{range}
			\SetKwFunction{sqrtm}{sqrtm}
			\SetKwFunction{sylvester}{sylvester}
			\SetKwInOut{Input}{input}
			\SetKwInOut{Output}{output}

			\Input{A real upper triangular matrix $T$}
			\Input{The number of blocks in this diagonal $\#\mathrm{blocks}$}
			\Input{The dimension of a full block $b$}
			\Output{A real upper triangular matrix $U$, where $U^2 \approx T$}
			$n \leftarrow$ dimension of $T$\;
			$\#\mathrm{blocks} \leftarrow \ceil{n / b}$\;
			
			\For{$e \leftarrow 0$ \KwTo $\#\mathrm{blocks}-1$}{
				$i_0 \leftarrow e \cdot b$\;
				$i_1 \leftarrow $\min{$(e+1) \cdot b$, $n$}$-1$\;
				$i \leftarrow $\range{$i_0$, $i_1$}\;
				\eIf{$d = 0$}{
					$U_{ii} \leftarrow $\sqrtm{${T_{ii}}$}\;
				}{
					$j_0 \leftarrow (e + d) \cdot b$\;
					$j_1 \leftarrow $\min{$(e + d + 1) \cdot b$, $n$}$-1$\;
					$j \leftarrow $\range{$j_0$, $j_1$}\;
					$F \leftarrow U_{ii}$\;
					$G \leftarrow U_{jj}$\;
					$C \leftarrow T_{ij}$\;
					\For{$z \leftarrow 1$ \KwTo $d - 1$}{
						$k_0 \leftarrow (e + z) \cdot b$\;
						$k_1 \leftarrow (e + z + 1) \cdot b - 1$\;
						$k \leftarrow $\range{$k_0$, $k_1$}\;
						$C \leftarrow C - U_{ik} \times U_{kj}$\;
					}
					$U_{ij} \leftarrow $\sylvester{$U_{ii}$, $U_{jj}$, $C$}\;
				}
			}
		\end{algorithm}

		\subsection{Armadillo}
		Documents published by \intel\cite{Intel:MIC:Overview} state that the best way to prepare for \intel\xeonphi coprocessors is to fully exploit the performance that an application can get on \intel\xeon processors first. This allows to use the \intel\vtune Amplifier to profile the implementation using the development multicore environment, where no coprocessor is available.

		\tdi{Describe how to obtain the hotspot analysis}
		\tdi{Show the hotspot analysis results, in particular that the bottleneck is in the armadillo matrix copy}

		The Armadillo library tries to minimize matrix allocations and copies whenever possible. For example, chaining matrix addition operations use a complex system of template ``glues'' that solve these additions without performing additional memory allocations. It also cares to use \blas operations without allocating a matrix to store the result whenever possible. Yet, it is unable to perform optimizations like these when blocks are isolated because these have to be treated as standalone matrices from then on. Also, Armadillo lacks an interface for the \lapack function that solves the Sylvester equation where the result matrix is not allocated. Consequently, it is necessary to remove Armadillo from the implementation, replacing it with standard arrays and manual calls to \blas and \lapack.

		For simplicity, implementation is bound to the \intel\mkl\blas interface. In the functions arguments (\cref{alg:mic:diagonal:point,alg:mic:diagonal:point:0,alg:mic:diagonal:point:1,alg:mic:diagonal:point:n,alg:mic:diagonal:block,alg:mic:diagonal:block:0,alg:mic:diagonal:block:1,alg:mic:diagonal:block:n}), Armadillo matrices are replaced with standard memory pointers and the matrix dimension $n$. In the point method, $s \leftarrow r \times c$ is replaced with a call to \blas\texttt{DOT}, which using increments of $1$ and $n$, respectively, does not require the $c$ and $r$ arrays to be isolated.

		For the block method, $C \leftarrow C - U_{ik} \times U_{kj}$ is replaced with a call to \blas\texttt{GEMM}, and $U_{ij} \leftarrow \mathtt{sylvester} \left( U_{ii}, U_{jj}, C\right)$ is replaced with a call to \lapack\texttt{TRSYL}. Both these calls overwrite one of the operands, effectively removing any need for allocations and copy operations.

		Note that while Armadillo is removed from the computation, it is still used for I/O operations for loading the matrix from a file and outputting the result.

		\subsection{Blocks as matrices}
		\label{subsec:blockify}
		After removing Armadillo, it becomes clear how \blas and \lapack calls access sub-matrices using the leading dimensions of the whole matrix. This simple approach is, however, error prone and better locality could be achieved were it not required to jump $n$ elements from one block column to the next.

		The matrices can be reorganized so each independent block is contiguous in memory, effectively making it an independent matrix. See \cref{eq:blockify} for example. $A$ is a regular column-major matrix, with the elements in the same column contiguous in memory, and each column also contiguous in memory. When trying to access the sub-matrix $A$ corresponding to the first two rows and columns, three elements of the first column must be skipped. In matrices where the dimension is large enough this translates into one memory access per block column. ``Blockifying'' $A$ generates $B$ where this does not happen because each block is now a column-major matrix, with all the blocks in the same column contiguous in memory, and the same being true for all the columns of blocks.

		\begin{equation}
			\begin{array}{ccc}
				\left[
				\begin{array}{c|c|c|c|c}
					 1 &  2 &  4 &  7 & 11  \\
					 0 &  3 &  5 &  8 & 12  \\
					 0 &  0 &  6 &  9 & 13  \\
					 0 &  0 &  0 & 10 & 14  \\
					 0 &  0 &  0 &  0 & 15  \\
				\end{array}
				\right] & \Rightarrow & \left[
				\begin{array}{cc|cc|c}
					 1 &  2 &  4 &  7 & 11  \\
					 0 &  3 &  5 &  8 & 12  \\
					 \hline
					 0 &  0 &  6 &  9 & 13  \\
					 0 &  0 &  0 & 10 & 14  \\
					 \hline
					 0 &  0 &  0 &  0 & 15  \\
				\end{array}
				\right] \\
				A & & B
			\end{array}
			\label{eq:blockify}
		\end{equation}

		This blockify operation does add some overhead to the initialization and to the cleanup (to revert the result to standard format), but it improves cache usage by through spatial locality. This overhead may or may not be worth depending on how good is this improvement and how it affects the computation.

		\subsection{Single matrix}
		All the implementations so far assume at least two distinct matrices are used in the algorithm, one for $T$ and another one for $U$. Aside from dependencies, this implies that when a block $U_{ij}$ is being computed, another block $T_{ij}$ must also be present. The memory footprint becomes even larger with the optimization described in \cref{subsec:blockify}. ``Blockifying'' the input matrix generates a second, re-organized matrix, which is then used as the input matrix for the algorithm. The algorithm then generates a third matrix, also ``blockified'', with the result, which then has to be re-organized into a fourth standard matrix with the final result.

		\blas and \lapack calls minimize the memory footprint by overwriting one of the operands with the result of the operation. In the case of the matrix square root algorithm this is also possible since only one element/block in the input matrix $T$ is used in the computation of each element/block in $U$. Consequently, the memory footprint can be easily reduced by overwriting $T$ with $U$.
	\section{Results}
\end{document}
