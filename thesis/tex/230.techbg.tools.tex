\documentclass[../thesis]{subfiles}

\begin{document}
		\section{Development Tools}
		\label{sec:techbg:tools}

		\tdg{new programming paradigm}%
		Most developers use conventional sequential programming models, as this is the most natural way to plan the resolution of a given problem, one step at a time. For single-core systems, this worked perfectly, with the only parallelism in applications being extracted by the compiler and the processor at the instruction level. The transition to the multicore era brought together a new programming paradigm, which must be understood in order to fully take advantage of the most modern computing resources available.

		\tdg{parallel programming requires new tools, methods and deeper arch knowledge}%
		Making the transition to parallel programming is not trivial. The ability to concurrently run several execution threads exposes the programmer to new problems: data races, workload balancing, deadlocks, etc. Debugging parallel applications is also harder and it requires smarter approaches, better than simply tracing the code (anything with more than four threads will be very hard to keep track of). The problem becomes even more complex when trying to increase efficiency with \hetplats. Often, a developer must be aware of the underlying architectural details in order to deploy an efficient implementation.

		\tdg{development tools}%
		Several tools have been presented to aid developers in taking advantage of the resources available in multicore shared memory environments and \hetplats, namely to distribute data and workloads among all available computing resources and to manage efficient data transfers across private memory spaces. Despite none being explored in the scope of this dissertation, many frameworks have also been developed to abstract the programmer from architectural details and the complexities of adapting code to run in a new platform (like a hardware accelerator).

		\subsection{PThreads, OpenMP, TBB and Cilk}

		\tdg{pthreads}
		\pthreads names the standard C language threads programming interface for UNIX systems. This standard was introduced with the goal of making parallel programming in shared-memory systems portable when hardware vendors implemented their own proprietary versions of threads \cite{LLNL:Barney:pthreads}. This API provides the tools for managing threads, mutual exclusion, condition variables, read/write locks and per-thread context.

		\tdg{OpenMP origin and purpose}
		OpenMP \cite{OpenMP,OpenMP:spec3.1} was formulated under a need similar to the purpose of \pthreads: to abstract the different ways operating systems imposed for programming with threads. At the time (1997), UNIX used \pthreads, Sun used Solaris threads, Windows used its own API and Linux used Linux threads \cite{Intel:pthreads_or_openmp}.

		\tdg{OpenMP}
		OpenMP is a standard API, in C/C++ and Fortran, for parallel programming in a multi-platform shared memory application running on all architectures. The API itself is less verbose than \pthreads and is very simple and easy to use (often through compiler directives). It abstracts an inexperience programmer from all the complexity of managing threads, but without lacking the required tools for advanced users to perform fine tuning. It is also portable and scalable. OpenMP only addresses homogeneous systems with conventional \cpus and automatically schedules efficient workloads among all available resources.

		\tdg{TBB}
		\intel\tbb is a C++ template library created by \intel with a similar purpose to OpenMP. While it is a lot more verbose, and lacks support for other languages, \tbb provides algorithms, highly concurrent containers, locks and atomic operations, a task scheduler and a scalable memory allocator \cite{TBB}. It is harder to program than OpenMP, but \intel claims it achieves equivalent or better performance.

		\subsection{OpenMPC and OpenACC}

		\tdg{OpenMPC}
		OpenMPC \cite{OpenMPC} is an extension of the OpenMP specification to provide translation from regular OpenMP compiler directives to \cuda code. Parallel zone directives delimit the blocks of code candidate for \cuda kernels. Only loop and section directives are considered true parallel sections, which are translated to perform workload distribution among the available threads. Synchronization directives cause the kernels to be split, as this is the only way to force global synchronization among all threads. Directives specifying data properties are interpreted to find the best \gpu memory space for the required data.

		\tdg{OpenACC}
		OpenACC \cite{OpenACC:2.0} is a standard API, in the same languages as OpenMP, meant to bring the advantages of OpenMP to programming with hardware accelerators. While originally designed only for \gpus, support has been extended for the \intel\xeonphi coprocessor. It abstracts the programmer from the memory management, kernel creation and the accelerator management. It also allows to execute both on the device and the host at the same time.

		\tdg{OpenMPC vs OpenACC}
		Comparing, OpenMPC only provides support only for \cuda-enabled devices, while OpenACC supports \nvidia and \amd\gpus alike and \intel\mic devices.
\end{document}
